{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación y Comparativa de Modelos de Lenguaje a Gran Escala (LLM) en Local para Arquitecturas RAG: Efectividad, Privacidad y Autonomía\n",
    "\n",
    "Este notebook constituye el espacio de trabajo donde se han desarrollado y ejecutado los experimentos presentados en este estudio. Aquí se integra todo el marco experimental, desde la configuración inicial del entorno hasta la ejecución y análisis de los flujos RAG, utilizando las interfaces diseñadas específicamente para este propósito. \n",
    "\n",
    "En cada sección se abordan las distintas etapas del experimento, incluyendo la carga de datos, la construcción de pipelines RAG y la evaluación mediante las métricas definidas en el marco RAGAs. Este notebook no solo documenta los resultados obtenidos, sino que también sirve como referencia técnica para replicar y ajustar los experimentos en futuros estudios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del Entorno\n",
    "\n",
    "Antes de ejecutar este notebook, asegúrate de que tu entorno está correctamente configurado para evitar problemas de compatibilidad o interferencias con otros proyectos. A continuación, te indicamos los pasos necesarios para el set up:\n",
    "\n",
    "1. **Instalación de Python 3.12**:  \n",
    "   Este notebook requiere Python 3.12. Descárgalo e instálalo desde [python.org](https://www.python.org/).\n",
    "\n",
    "2. **Crear un entorno virtual (recomendado)**:\n",
    "    Para evitar conflictos con otras configuraciones de Python, se recomienda crear un entorno virtual. Puedes hacerlo con los siguientes comandos:\n",
    "\n",
    "    ```\n",
    "    py -3.12 -m venv venv\n",
    "    source venv/bin/activate  # En Linux/Mac\n",
    "    venv\\Scripts\\activate     # En Windows\n",
    "    ```\n",
    "\n",
    "3. **Instalación de iKernel**:  \n",
    "   Para utilizar Python 3.12 como motor del notebook, necesitas instalar el módulo `ipykernel`. Esto puede hacerse directamente desde el entorno virtual mediante el siguiente comando:  \n",
    "\n",
    "   ```\n",
    "   pip install ipykernel\n",
    "   ```\n",
    "\n",
    "   Asegúrate de añadir este kernel al notebook una vez instalado.\n",
    "\n",
    "4. **Instalación de dependencias**:\n",
    "    Las dependencias necesarias para este notebook se encuentran en la celda de configuración. Estas se instalarán directamente desde el notebook mediante `%pip install...` Si las instalas por primera vez, es posible que necesites reiniciar el kernel para que surtan efecto.\n",
    "\n",
    "**Nota importante:** Durante la instalación de las librerías necesarias en este entorno virtual, es posible que aparezcan mensajes relacionados con incompatibilidades entre algunas dependencias. Por ejemplo, conflictos menores entre las versiones de `tenacity`, `grpcio`, `protobuf` y `deepeval`. Estas incompatibilidades son inherentes a las dependencias actuales de las librerías utilizadas en este notebook, pero hemos verificado que no afectan su ejecución ni los resultados esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## CONFIGURACIÓN DEL ENTORNO: \n",
    "# Instalación de paquetes y librerías necesarias\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Update pip\n",
    "%pip install -qU pip\n",
    "\n",
    "# Langchain Installation\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# PDF Loaders Libraries\n",
    "%pip install -qU pymupdf \n",
    "%pip install -qU langchain-unstructured \"langchain-unstructured[local]\" unstructured-client unstructured \"unstructured[pdf]\" \n",
    "%pip install -qU azure-ai-documentintelligence azure-core\n",
    "%pip install -qU langchain-text-splitters\n",
    "\n",
    "# Embeddings Models Libraries\n",
    "%pip install -qU langchain_ollama\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain_voyageai\n",
    "%pip install -qU langchain_mistralai\n",
    "\n",
    "# Vector Store Libraries\n",
    "%pip install -qU langchain_chroma\n",
    "%pip install -qU langchain-mongodb pymongo\n",
    "\n",
    "# Large Language Models Libraries\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain_anthropic\n",
    "%pip install -qU langchain-google-genai\n",
    "%pip install -qU langchain_mistralai\n",
    "\n",
    "# Evaluation Libraries\n",
    "%pip install -qU deepeval\n",
    "%pip install -qU instructor\n",
    "\n",
    "# Other Libraries\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU ipywidgets\n",
    "%pip install -qU tqdm\n",
    "%pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de las Variables de Entorno\n",
    "\n",
    "Este notebook requiere varias claves de API y configuraciones específicas. Por favor, copia el archivo `.env_sample`, renómbralo a `.env` y edítalo con tus claves y configuraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## CONFIGURACIÓN DE LAS VARIABLES DE ENTORNO: \n",
    "# Carga de variables de entorno desde el archivo .env\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz para los Módulos de Vectorización\n",
    "\n",
    "En este apartado, construiremos nuestra interfaz de embeddings personalizada en base a los conectores de embeddings proporcionados por LangChain. Gracias a que LangChain actúa como capa de abstracción, todos los conectores funcionan de manera homogénea, lo que nos permite intercambiarlos sin complicaciones. De esta forma, podremos cargar dinámicamente distintos módulos de embeddings según lo definan nuestras variables, facilitando la integración y flexibilidad en nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Vectorización\n",
    "# Nomic Embeddings: nomic-embed-text - LOCAL [768]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model nomic-embed-text from https://ollama.com/library/nomic-embed-text:latest\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "def myNomicEmbedder():\n",
    "    embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embedder\n",
    "\n",
    "## # Test Embedder\n",
    "## input_text = \"The meaning of life is 42\"\n",
    "## embedder = myNomicEmbedder()\n",
    "## vector = embedder.embed_query(input_text)\n",
    "## print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Vectorización\n",
    "# Snowflake Embeddings: snowflake-arctic-embed:335m - LOCAL [1024]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model snowflake-arctic-embed from https://ollama.com/library/snowflake-arctic-embed\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "def mySnowflakeArcticEmbedder():\n",
    "    embedder = OllamaEmbeddings(model=\"snowflake-arctic-embed:335m\")\n",
    "    return embedder\n",
    "\n",
    "## # Test Embedder\n",
    "## input_text = \"The meaning of life is 42\"\n",
    "## embedder = mySnowflakeArcticEmbedder()\n",
    "## vector = embedder.embed_query(input_text)\n",
    "## print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Vectorización\n",
    "# OpenAI Embeddings: text-embedding-3-large - CLOUD [3072]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in OpenAI API Platform: https://platform.openai.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def myOpenAiEmbedder():\n",
    "    embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    return embedder\n",
    "\n",
    "## # Test Embedder\n",
    "## input_text = \"The meaning of life is 42\"\n",
    "## embedder = myOpenAiEmbedder()\n",
    "## vector = embedder.embed_query(input_text)\n",
    "## print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Vectorización\n",
    "# Voyage AI Embeddings: voyage-3 - CLOUD [1024]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Voyage AI Platform: https://www.voyageai.com\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "\n",
    "import os\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "def myVoyageAiEmbedder():\n",
    "    embedder = VoyageAIEmbeddings(voyage_api_key=os.getenv(\"VOYAGEAI_API_KEY\"), model=\"voyage-3\")\n",
    "    return embedder\n",
    "\n",
    "## # Test Embedder\n",
    "## input_text = \"The meaning of life is 42\"\n",
    "## embedder = myVoyageAiEmbedder()\n",
    "## vector = embedder.embed_query(input_text)\n",
    "## print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Vectorización\n",
    "# Mistral AI Embeddings: mistral-embed - CLOUD [1024]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Mistral AI Platform: https://console.mistral.ai\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "4. You might need as well a Hugging Face Token. Set it up in the .env file\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langchain_mistralai.embeddings\")\n",
    "\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "def myMistralAiEmbedder():\n",
    "    embedder = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    return embedder\n",
    "\n",
    "## # Test Embedder\n",
    "## input_text = \"The meaning of life is 42\"\n",
    "## embedder = myMistralAiEmbedder()\n",
    "## vector = embedder.embed_query(input_text)\n",
    "## print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE VECTORIZACIÓN:\n",
    "# Interfaz personalizada para definir de manera dinámica el modulo de \n",
    "# vectorización.\n",
    "\n",
    "# Modelos Disponibles:\n",
    "# - nomic-embed-text\n",
    "# - snowflake-arctic-embed-335m\n",
    "# - text-embedding-3-large\n",
    "# - voyage-3\n",
    "# - mistral-embed\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "class MyEmbedder:\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "        self.dimension = {\n",
    "            \"nomic-embed-text\": 768,\n",
    "            \"snowflake-arctic-embed-335m\": 1024,\n",
    "            \"text-embedding-3-large\": 3072,\n",
    "            \"voyage-3\": 1024,\n",
    "            \"mistral-embed\": 1024\n",
    "        }.get(model, None)\n",
    "        \n",
    "    def get_embedder(self):\n",
    "        if self.model == \"nomic-embed-text\":\n",
    "            return myNomicEmbedder()\n",
    "        elif self.model == \"snowflake-arctic-embed-335m\":\n",
    "            return mySnowflakeArcticEmbedder()\n",
    "        elif self.model == \"text-embedding-3-large\":\n",
    "            return myOpenAiEmbedder()\n",
    "        elif self.model == \"voyage-3\":\n",
    "            return myVoyageAiEmbedder()\n",
    "        elif self.model == \"mistral-embed\":\n",
    "            return myMistralAiEmbedder()\n",
    "        else:\n",
    "            raise ValueError(f\"Model {self.model} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE VECTORIZACIÓN:\n",
    "# Prueba de la interfaz personalizada para los módulos de vectorización.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Define the embedding model to use\n",
    "# Options: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3', 'mistral-embed'\n",
    "embedding_model = 'text-embedding-3-large'\n",
    "embedder = MyEmbedder(model=embedding_model).get_embedder()\n",
    "\n",
    "# Test the embedder\n",
    "input_text = 'The meaning of life is 42'\n",
    "embedding = embedder.embed_query(input_text)\n",
    "print(f'Lenght of the vector: {len(embedding)}.\\nFirst 3 elements: {embedding[:3]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz para los Módulos de Bases de Datos de Vectores\n",
    "\n",
    "En esta sección, definiremos nuestras bases de datos de vectores y los retrievers asociados utilizando los conectores disponibles en LangChain. Aunque todos los conectores de bases de datos de vectores comparten una interfaz común, presentan diferencias en cómo filtran o eliminan la información, lo que nos obliga a implementar algunas variaciones en estos procesos. A pesar de estas diferencias, LangChain sigue actuando como una capa de abstracción que facilita la integración y el intercambio de bases de datos de vectores, permitiéndonos cargarlas dinámicamente según lo definan nuestras variables y garantizando la flexibilidad en nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Bases de Datos de Vectores\n",
    "# Chroma Vector Store - LOCAL\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. The Chroma Vector Store is a local database for storing and querying vectors.\n",
    "2. The objects are stored by default in the directory \"../ChromaDb\".\n",
    "'''\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def myChromaDbVectorStore(Embedder: MyEmbedder):\n",
    "    vector_store = Chroma(\n",
    "        collection_name=f'collection-{Embedder.model}',\n",
    "        embedding_function=Embedder.get_embedder(),\n",
    "        persist_directory=\"../ChromaDb\",\n",
    "        # Available functions: 'l2', 'cosine', and 'ip'\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"} \n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Bases de Datos de Vectores\n",
    "# MongoDB Atlas Vector Store - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in MongoDB Atlas: https://www.mongodb.com/cloud/atlas\n",
    "2. Create a Cluster\n",
    "3. Update the .env file with the MongoDB User, Password, and Cluster URI\n",
    "4. First time setting up a collection, set create_index=True\n",
    "'''\n",
    "\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "def myMongoDbAtlasVectorStore(Embedder: MyEmbedder, create_index: bool = False):\n",
    "    client = MongoClient(os.getenv(\"MONGO_DB_CLUSTER_URI\"))\n",
    "\n",
    "    DB_NAME = \"myMongoDbAtlasVectorStore\"\n",
    "    COLLECTION_NAME = f\"collection-{Embedder.model}\"\n",
    "    ATLAS_VECTOR_SEARCH_INDEX_NAME = f\"index-{Embedder.model}\"\n",
    "\n",
    "    MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "    vector_store = MongoDBAtlasVectorSearch(\n",
    "        collection=MONGODB_COLLECTION,\n",
    "        embedding=Embedder.get_embedder(),\n",
    "        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "        # Available functions: 'euclidean', 'cosine', and 'dotProduct'\n",
    "        relevance_score_fn=\"cosine\", \n",
    "    )\n",
    "\n",
    "    # Create the vector search index - Needs to be done on the initial setup\n",
    "    if create_index:\n",
    "        vector_store.create_vector_search_index(dimensions=Embedder.dimension, filters=[\"source\", \"parser\"])\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE BASES DE DATOS DE VECTORES:\n",
    "# Interfaz personalizada para definir de manera dinámica el modulo de base \n",
    "# de datos de vectores.\n",
    "\n",
    "# Proveedores Disponibles:\n",
    "# - chromadb\n",
    "# - mongodb\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "class MyVectorStore:\n",
    "\n",
    "    def __init__(self, provider: str, Embedder: MyEmbedder):\n",
    "        self.provider = provider\n",
    "        self.Embedder = Embedder\n",
    "    \n",
    "    def get_vector_store(self):\n",
    "        if self.provider == \"chromadb\":\n",
    "            return myChromaDbVectorStore(Embedder=self.Embedder)\n",
    "        elif self.provider == \"mongodb\":\n",
    "            return myMongoDbAtlasVectorStore(Embedder=self.Embedder)\n",
    "        else:\n",
    "            raise ValueError(f\"Provider {self.provider} not supported\")\n",
    "\n",
    "    def get_vector_store_documents(self, source: str, parser: str):\n",
    "        vector_store = self.get_vector_store()\n",
    "\n",
    "        if self.provider == \"chromadb\":\n",
    "            old_docs = vector_store.get(where={\"$and\": [{\"source\": source}, {\"parser\": parser}]})\n",
    "            return old_docs[\"ids\"]\n",
    "        \n",
    "        elif self.provider == \"mongodb\":\n",
    "            old_docs = list(vector_store._collection.find({\"source\": source, \"parser\": parser}, {\"_id\": 1}))\n",
    "            return [doc[\"_id\"] for doc in old_docs]\n",
    "\n",
    "    def purge_vector_store_documents(self, source: str, parser: str):\n",
    "        vector_store = self.get_vector_store()\n",
    "        document_ids = self.get_vector_store_documents(source=source, parser=parser)\n",
    "        if document_ids:\n",
    "            vector_store.delete(ids=document_ids)\n",
    "    \n",
    "    def push_vector_store_documents(self, documents: list, embeddings: list = None):\n",
    "        vector_store = self.get_vector_store()\n",
    "        if not embeddings:\n",
    "            vector_store.add_documents(documents=documents)\n",
    "            return\n",
    "        \n",
    "        if self.provider == \"chromadb\":\n",
    "            vector_store._collection.add(\n",
    "                ids=[str(uuid4()) for _ in range(len(documents))],\n",
    "                documents=[doc.page_content for doc in documents],\n",
    "                embeddings=embeddings,\n",
    "                metadatas=[doc.metadata for doc in documents]\n",
    "            )\n",
    "        \n",
    "        elif self.provider == \"mongodb\":\n",
    "            vector_store._collection.insert_many([\n",
    "                {\n",
    "                    \"text\" : text,\n",
    "                    \"embedding\" : embedding,\n",
    "                    **metadata\n",
    "                } for text, embedding, metadata in zip(\n",
    "                    [doc.page_content for doc in documents], \n",
    "                    embeddings, \n",
    "                    [doc.metadata for doc in documents]\n",
    "                )\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE RETRIEVAL:\n",
    "# Interfaz personalizada para definir de manera dinámica el modulo de \n",
    "# retrieval.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "class MyRetriever:\n",
    "\n",
    "    def __init__(self, k: int, source: str, parser: str, VectorStore: MyVectorStore):\n",
    "        self.k = k\n",
    "        self.source = source\n",
    "        self.parser = parser\n",
    "        self.VectorStore = VectorStore\n",
    "\n",
    "    def get_retriever(self):\n",
    "        vector_store = self.VectorStore.get_vector_store()\n",
    "        if self.VectorStore.provider == \"chromadb\":\n",
    "            return vector_store.as_retriever(search_type='similarity', search_kwargs={'k': self.k, 'filter': {\"$and\": [{\"source\": self.source}, {\"parser\": self.parser}]}})\n",
    "        elif self.VectorStore.provider == \"mongodb\":\n",
    "            return vector_store.as_retriever(search_type='similarity', search_kwargs={'k': self.k, 'pre_filter': {\"source\": {\"$eq\": self.source}, \"parser\": {\"$eq\": self.parser}}})\n",
    "        else:\n",
    "            raise ValueError(f\"Provider {self.provider} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE BASES DE DATOS DE VECTORES:\n",
    "# Prueba de la interfaz personalizada para los módulos de base de \n",
    "# datos de vectores - Carga de documentos.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define the documents to add to the vector store\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"test\", \"parser\": \"test\"}\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"test\", \"parser\": \"test\"}\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"There has been a significant increase in the number of COVID-19 cases in the past week.\",\n",
    "    metadata={\"source\": \"test\", \"parser\": \"test\"}\n",
    ")\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "# Load the documents into the vector stores\n",
    "# Available providers: 'chromadb', 'mongodb'\n",
    "for db_provider in [\"chromadb\", \"mongodb\"]:\n",
    "    # Using all the possible embedding models\n",
    "    # Available models: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3', 'mistral-embed'\n",
    "    for embedding_model in [\"nomic-embed-text\", \"snowflake-arctic-embed-335m\", \"text-embedding-3-large\", \"voyage-3\", \"mistral-embed\"]:\n",
    "        \n",
    "        # Define the Embedder and Vector Store\n",
    "        Embedder = MyEmbedder(model=embedding_model)\n",
    "        VectorStore = MyVectorStore(provider=db_provider, Embedder=Embedder)\n",
    "\n",
    "        # Purge Existing Documents\n",
    "        VectorStore.purge_vector_store_documents(source=\"test\", parser=\"test\")\n",
    "        \n",
    "        # Embed the Documents\n",
    "        embedder = Embedder.get_embedder()\n",
    "        embeddings = [embedder.embed_query(doc.page_content) for doc in documents]\n",
    "        \n",
    "        # Push the Documents\n",
    "        VectorStore.push_vector_store_documents(documents=documents, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE RETRIEVAL:\n",
    "# Prueba de la interfaz personalizada para los módulos de retrieval - Busqueda \n",
    "# de documentos.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Define the Embedder\n",
    "# Available models: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3', 'mistral-embed'\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "Embedder = MyEmbedder(model=embedding_model)\n",
    "\n",
    "# Define the Vector Store\n",
    "# Available providers: 'chromadb', 'mongodb'\n",
    "db_provider = \"mongodb\"\n",
    "VectorStore = MyVectorStore(provider=db_provider, Embedder=Embedder)\n",
    "\n",
    "# Define the Retriever and its filters\n",
    "k, source, parser = 2, \"test\", \"test\"\n",
    "retriever = MyRetriever(k=k, source=source, parser=parser, VectorStore=VectorStore).get_retriever()\n",
    "\n",
    "# Test the retriever\n",
    "query = \"Will it be hot tomorrow?\"\n",
    "docs = retriever.invoke(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz para los Módulos de LLMs\n",
    "\n",
    "En esta sección, desarrollaremos nuestra interfaz personalizada para los LLMs utilizando los conectores disponibles a través de LangChain. Al igual que con los embeddings, LangChain actúa como una capa de abstracción, lo que garantiza que todos los conectores de LLMs funcionen de manera homogénea. Esto nos permite intercambiar entre distintos modelos de lenguaje sin dificultades. Gracias a esta flexibilidad, podemos cargar dinámicamente diferentes modelos de LLM según lo definan nuestras variables, facilitando la integración y adaptación de nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# OpenAI: gpt-4o - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in OpenAI API Platform: https://platform.openai.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def myOpenAiLLM_gpt_4o(temperature: float):\n",
    "    return ChatOpenAI(model=\"gpt-4o\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myOpenAiLLM_gpt_4o(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# OpenAI: o1-preview - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in OpenAI API Platform: https://platform.openai.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def myOpenAiLLM_o1(temperature: float):\n",
    "    return ChatOpenAI(model=\"o1-preview\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myOpenAiLLM_o1(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Anthropic: claude-3-5-haiku - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Anthropic AI Platform: https://www.anthropic.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def myAnthropicLLM_claude_3_5_haiku(temperature: float):\n",
    "    return ChatAnthropic(model=\"claude-3-5-haiku-latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myAnthropicLLM_claude_3_5_haiku(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Anthropic: claude-3-5-sonnet - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Anthropic AI Platform: https://www.anthropic.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def myAnthropicLLM_claude_3_5_sonnet(temperature: float):\n",
    "    return ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myAnthropicLLM_claude_3_5_sonnet(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Anthropic: claude-3-opus - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Anthropic AI Platform: https://www.anthropic.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def myAnthropicLLM_claude_3_opus(temperature: float):\n",
    "    return ChatAnthropic(model=\"claude-3-opus-latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myAnthropicLLM_claude_3_opus(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Google: gemini-1.5-flash - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Google API Platform: https://aistudio.google.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def myGoogleLLM_gemini_1_5_flash(temperature: float):\n",
    "    return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myGoogleLLM_gemini_1_5_flash(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Google: gemini-1.5-pro - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Google API Platform: https://aistudio.google.com/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def myGoogleLLM_gemini_1_5_pro(temperature: float):\n",
    "    return ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myGoogleLLM_gemini_1_5_pro(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Google: gemma2:2b - LOCAL [1.6Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model gemma2:2b from https://ollama.com/library/gemma2\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myGoogleLLM_gemma2_2b(temperature: float):\n",
    "    return ChatOllama(model=\"gemma2:2b\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myGoogleLLM_gemma2_2b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Google: gemma2:9b - LOCAL [5.4Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model gemma2:9b from https://ollama.com/library/gemma2\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myGoogleLLM_gemma2_9b(temperature: float):\n",
    "    return ChatOllama(model=\"gemma2:latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myGoogleLLM_gemma2_9b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Google: gemma2:27b - LOCAL [16Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model gemma2:27b from https://ollama.com/library/gemma2\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myGoogleLLM_gemma2_27b(temperature: float):\n",
    "    return ChatOllama(model=\"gemma2:27b\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myGoogleLLM_gemma2_27b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Meta: llama3.2:1b - LOCAL [1.3Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model llama3.2:1b from https://ollama.com/library/llama3.2\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myMetaLLM_llama3_2_1b(temperature: float):\n",
    "    return ChatOllama(model=\"llama3.2:1b\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMetaLLM_llama3_2_1b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Meta: llama3.2:3b - LOCAL [2Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model llama3.2:3b from https://ollama.com/library/llama3.2\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myMetaLLM_llama3_2_3b(temperature: float):\n",
    "    return ChatOllama(model=\"llama3.2:3b\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMetaLLM_llama3_2_3b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Meta: llama3.1:8b - LOCAL [4.7Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model llama3.1:8b from https://ollama.com/library/llama3.1\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myMetaLLM_llama3_1_8b(temperature: float):\n",
    "    return ChatOllama(model=\"llama3.1:8b\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMetaLLM_llama3_1_8b(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Nvidia: nemotron-mini - LOCAL [2.7Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model nemotron-mini from https://ollama.com/library/nemotron-mini\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myNvidiaLLM_nemotron_mini(temperature: float):\n",
    "    return ChatOllama(model=\"nemotron-mini:latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myNvidiaLLM_nemotron_mini(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Microsoft: phi3.5 - LOCAL [2.2Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model phi3.5 from https://ollama.com/library/phi3.5\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myMicrosoftLLM_phi3_5(temperature: float):\n",
    "    return ChatOllama(model=\"phi3.5:latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMicrosoftLLM_phi3_5(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Mistral AI: mistral-large - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Mistral AI Platform: https://console.mistral.ai\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "4. You might need as well a Hugging Face Token. Set it up in the .env file\n",
    "'''\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "def myMistralLLM_mistral_large(temperature: float):\n",
    "    return ChatMistralAI(model=\"mistral-large-latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMistralLLM_mistral_large(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de LLM\n",
    "# Mistral AI: mistral-small - LOCAL [13Gb]\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Download Ollama from https://ollama.com\n",
    "2. Download the model mistral-small from https://ollama.com/library/mistral-small\n",
    "3. Start the Ollama server\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def myMistralLLM_mistral_small(temperature: float):\n",
    "    return ChatOllama(model=\"mistral-small:latest\", temperature=temperature)\n",
    "\n",
    "## # Test LLM\n",
    "## messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "## llm = myMistralLLM_mistral_small(temperature=1)\n",
    "## response = llm.invoke(messages)\n",
    "## print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE LLM:\n",
    "# Interfaz personalizada para definir de manera dinámica el modulo de LLM.\n",
    "\n",
    "# Modelos Disponibles:\n",
    "# - gpt-4o, o1-preview\n",
    "# - claude-3-5-haiku, claude-3-5-sonnet, claude-3-opus\n",
    "# - gemini-1.5-flash, gemini-1.5-pro\n",
    "# - gemma2-2b, gemma2-9b, gemma2-27b\n",
    "# - llama3-2-1b, llama3-2-3b, llama3-1-8b\n",
    "# - nemotron-mini\n",
    "# - phi3-5\n",
    "# - mistral-large, mistral-small\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "class MyLLM:\n",
    "\n",
    "    def __init__(self, model: str, temperature: float):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def get_llm(self):\n",
    "        if self.model == \"gpt-4o\":\n",
    "            return myOpenAiLLM_gpt_4o(temperature=self.temperature)\n",
    "        elif self.model == \"o1-preview\":\n",
    "            return myOpenAiLLM_o1(temperature=self.temperature)\n",
    "        elif self.model == \"claude-3-5-haiku\":\n",
    "            return myAnthropicLLM_claude_3_5_haiku(temperature=self.temperature)\n",
    "        elif self.model == \"claude-3-5-sonnet\":\n",
    "            return myAnthropicLLM_claude_3_5_sonnet(temperature=self.temperature)\n",
    "        elif self.model == \"claude-3-opus\":\n",
    "            return myAnthropicLLM_claude_3_opus(temperature=self.temperature)\n",
    "        elif self.model == \"gemini-1.5-flash\":\n",
    "            return myGoogleLLM_gemini_1_5_flash(temperature=self.temperature)\n",
    "        elif self.model == \"gemini-1.5-pro\":\n",
    "            return myGoogleLLM_gemini_1_5_pro(temperature=self.temperature)\n",
    "        elif self.model == \"gemma2-2b\":\n",
    "            return myGoogleLLM_gemma2_2b(temperature=self.temperature)\n",
    "        elif self.model == \"gemma2-9b\":\n",
    "            return myGoogleLLM_gemma2_9b(temperature=self.temperature)\n",
    "        # Not working in my laptop due to RAM limitations\n",
    "        ## elif self.model == \"gemma2-27b\":\n",
    "        ##     return myGoogleLLM_gemma2_27b(temperature=self.temperature)\n",
    "        elif self.model == \"llama3-2-1b\":\n",
    "            return myMetaLLM_llama3_2_1b(temperature=self.temperature)\n",
    "        elif self.model == \"llama3-2-3b\":\n",
    "            return myMetaLLM_llama3_2_3b(temperature=self.temperature)\n",
    "        elif self.model == \"llama3-1-8b\":\n",
    "            return myMetaLLM_llama3_1_8b(temperature=self.temperature)\n",
    "        elif self.model == \"nemotron-mini\":\n",
    "            return myNvidiaLLM_nemotron_mini(temperature=self.temperature)\n",
    "        elif self.model == \"phi3-5\":\n",
    "            return myMicrosoftLLM_phi3_5(temperature=self.temperature)\n",
    "        elif self.model == \"mistral-large\":\n",
    "            return myMistralLLM_mistral_large(temperature=self.temperature)\n",
    "        # Ollama distribution for this model is not working\n",
    "        ## elif self.model == \"mistral-small\":\n",
    "        ##     return myMistralLLM_mistral_small(temperature=self.temperature)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {self.model} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE LLM:\n",
    "# Prueba de la interfaz personalizada para los módulos de LLM.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Define the LLM model to use\n",
    "# Options: 'gpt-4o', 'o1-preview', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-opus', 'gemini-1.5-flash', 'gemini-1.5-pro', \n",
    "# 'gemma2-2b', 'gemma2-9b', 'llama3-2-1b', 'llama3-2-3b', 'llama3-1-8b', 'nemotron-mini', 'phi3-5', 'mistral-large'\n",
    "llm_model, temperature = \"gpt-4o\", 1\n",
    "llm = MyLLM(model=llm_model, temperature=temperature).get_llm()\n",
    "\n",
    "# Test the LLM\n",
    "messages = [(\"human\", \"What is the meaning of life?\")]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz para el Procesamiento de PDFs\n",
    "\n",
    "En esta sección, construiremos nuestra interfaz personalizada para el procesamiento de PDFs. Su función principal será extraer el contenido de un documento PDF y fragmentarlo en unidades de información, que luego serán cargadas en las bases de datos de vectores. Aunque LangChain proporciona algunos conectores para el procesamiento de PDFs, no todos los conectores están disponibles ni existe una interfaz estándar. Por lo tanto, seremos responsables de crear esta interfaz, homogenizando tanto la entrada como la salida, para asegurar que todos los módulos de procesamiento de PDFs funcionen de manera consistente en nuestros experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# PyMuPDF Loader - LOCAL\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def myPyMuPDFLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "    \n",
    "    loader = PyMuPDFLoader(file_path=path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(loader.load())\n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myPyMuPDFLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# Unstructured IO Loader - LOCAL\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Unstructured IO Platform: https://unstructured.io/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "4. Install Docker from https://www.docker.com/get-started\n",
    "5. Pull the Unstructured IO Docker image: docker pull downloads.unstructured.io/unstructured-io/unstructured-api:latest\n",
    "6. Run the Unstructured IO Docker image: docker run -p 9500:9500 -d --rm --name unstructured-api -e PORT=9500 downloads.unstructured.io/unstructured-io/unstructured-api:latest\n",
    "'''\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from unstructured_client import UnstructuredClient\n",
    "\n",
    "def myUnstructuredLocalLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "\n",
    "    client = UnstructuredClient(\n",
    "        server_url=\"http://localhost:9500\"\n",
    "    )\n",
    "    \n",
    "    loader = UnstructuredLoader(\n",
    "        file_path=path, \n",
    "        partition_via_api=True, \n",
    "        client=client, \n",
    "        strategy=\"hi_res\", \n",
    "        chunking_strategy=\"by_title\", \n",
    "        max_characters=chunk_size,\n",
    "        overlap=chunk_overlap,\n",
    "        include_orig_elements=False\n",
    "    )\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myUnstructuredLocalLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# Unstructured IO Loader - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Unstructured IO Platform: https://unstructured.io/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "import os\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from unstructured_client import UnstructuredClient\n",
    "\n",
    "def myUnstructuredCloudLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "\n",
    "    client = UnstructuredClient(\n",
    "        api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\"), \n",
    "        server_url=\"https://api.unstructuredapp.io/general/v0/general\"\n",
    "    )\n",
    "    \n",
    "    loader = UnstructuredLoader(\n",
    "        file_path=path, \n",
    "        partition_via_api=True, \n",
    "        client=client, \n",
    "        strategy=\"hi_res\", \n",
    "        chunking_strategy=\"by_title\", \n",
    "        max_characters=chunk_size,\n",
    "        overlap=chunk_overlap,\n",
    "        include_orig_elements=False\n",
    "    )\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myUnstructuredCloudLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# Azure Document Intelligence Loader - LOCAL\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Azure: https://portal.azure.com/#home\n",
    "2. Create a Document Intelligence Resource\n",
    "3. Set up the API Key and Endpoint URI in the .env file\n",
    "4. Install Docker from https://www.docker.com/get-started\n",
    "5. Pull and run the Azure Document Intelligence Docker image from ../AzDocInt: docker compose up\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def myAzureDocIntLocalLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "\n",
    "    # Extract text from the PDF\n",
    "    with open(path, \"rb\") as f:           \n",
    "        result = requests.post(\n",
    "            url = f\"{os.getenv('DOCUMENT_INTELLIGENCE_LOCAL_ENDPOINT_URI')}/formrecognizer/documentModels/prebuilt-layout:syncAnalyze?api-version=2023-07-31\",\n",
    "            headers={\"Content-Type\": \"application/octet-stream\"},\n",
    "            files={\"file\": f}\n",
    "        )\n",
    "        \n",
    "    result = json.loads(result.text)['analyzeResult']\n",
    "\n",
    "    # Split the text into documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    documents = [Document(page_content=content) for content in text_splitter.split_text(result['content'])]\n",
    "    return documents\n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myAzureDocIntLocalLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# Azure Document Intelligence Loader - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Azure: https://portal.azure.com/#home\n",
    "2. Create a Document Intelligence Resource\n",
    "3. Set up the API Key and Endpoint URI in the .env file\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, ContentFormat\n",
    "\n",
    "def myAzureDocIntCloudLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "    \n",
    "    # Extract text from the PDF\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=os.getenv('DOCUMENT_INTELLIGENCE_CLOUD_ENDPOINT_URI'), \n",
    "        credential=AzureKeyCredential(os.getenv('DOCUMENT_INTELLIGENCE_API_KEY'))\n",
    "    )\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", output_content_format=ContentFormat.MARKDOWN\n",
    "        )\n",
    "    \n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    # Split the text into documents\n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers = False)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    return text_splitter.split_documents(markdown_splitter.split_text(result.content)) \n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myAzureDocIntCloudLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## Interfaz para los Módulos de Procesamiento de PDFs\n",
    "# LlamaParse Loader - CLOUD\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "1. Create an account in Llama AI Platform: https://llamaindex.ai/\n",
    "2. Create an API Key\n",
    "3. Set up the API Key in the .env file\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def myLlamaParseLoader(path, chunk_size=500, chunk_overlap=50):\n",
    "\n",
    "    # Upload the file\n",
    "    with open(path, 'rb') as f:\n",
    "        response = requests.post(\n",
    "            url = 'https://api.cloud.llamaindex.ai/api/parsing/upload',\n",
    "            data={\n",
    "                'continuous_mode': True, \n",
    "                'premium_mode': True\n",
    "            },\n",
    "            headers={\n",
    "                'accept': 'application/json',\n",
    "                'Authorization': f'Bearer {os.getenv(\"LLAMA_CLOUD_API_KEY\")}'\n",
    "            },\n",
    "            files={'file': (os.path.basename(path), f, 'application/pdf')}\n",
    "        )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    job_id = response.json()['id']\n",
    "\n",
    "    # Check the status of the parsing job\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            url=f'https://api.cloud.llamaindex.ai/api/parsing/job/{job_id}',\n",
    "            headers={\n",
    "                'accept': 'application/json',\n",
    "                'Authorization': f'Bearer {os.getenv(\"LLAMA_CLOUD_API_KEY\")}'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        status = response.json()['status']\n",
    "        if status == 'SUCCESS':\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Get the results in Markdown\n",
    "    response = requests.get(\n",
    "        url=f'https://api.cloud.llamaindex.ai/api/parsing/job/{job_id}/result/markdown',\n",
    "        headers={\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {os.getenv(\"LLAMA_CLOUD_API_KEY\")}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    result = response.json()['markdown']\n",
    "\n",
    "    # Split the text into documents\n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers = False)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    return text_splitter.split_documents(markdown_splitter.split_text(result))\n",
    "\n",
    "## # Test Loader\n",
    "## path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "## documents = myLlamaParseLoader(path)\n",
    "## print(f\"Loaded {len(documents)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE PROCESAMIENTO DE PDFs:\n",
    "# Interfaz personalizada para definir de manera dinámica el modulo de \n",
    "# procesamiento de PDFs. \n",
    "\n",
    "# Métodos Disponibles:\n",
    "# - extract: Extrae el texto de un PDF utilizando el servicio seleccionado.\n",
    "# - load: Carga los documentos extraidos en el Vector Store seleccionado.\n",
    "\n",
    "# Servicios Disponibles:\n",
    "# - PyMuPDF\n",
    "# - UnstructuredLocal, UnstructuredCloud\n",
    "# - AzureDocIntLocal, AzureDocIntCloud\n",
    "# - LlamaParse\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "class MyPdfLoader:\n",
    "        \n",
    "        def __init__(self, path: str, loader: str, chunk_size=500, chunk_overlap=50):\n",
    "            self.path = path\n",
    "            self.loader = loader\n",
    "            self.chunk_size = chunk_size\n",
    "            self.chunk_overlap = chunk_overlap\n",
    "            self.source = os.path.basename(self.path)\n",
    "            \n",
    "        def extract(self):\n",
    "    \n",
    "            # Pdf Extraction\n",
    "            if self.loader == \"PyMuPDF\":\n",
    "                docs = myPyMuPDFLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            elif self.loader == \"UnstructuredLocal\":\n",
    "                docs = myUnstructuredLocalLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            elif self.loader == \"UnstructuredCloud\":\n",
    "                docs = myUnstructuredCloudLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            elif self.loader == \"AzureDocIntLocal\":\n",
    "                docs = myAzureDocIntLocalLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            elif self.loader == \"AzureDocIntCloud\":\n",
    "                docs = myAzureDocIntCloudLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            elif self.loader == \"LlamaParse\":\n",
    "                docs = myLlamaParseLoader(self.path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown loader: {self.loader}\")\n",
    "\n",
    "            return docs\n",
    "        \n",
    "        def load(self, VectorStore: MyVectorStore, documents: list = None, embeddings: list = None, replace: bool = False):\n",
    "\n",
    "            # Purge Existing Documents if replace=True\n",
    "            document_ids = VectorStore.get_vector_store_documents(source=self.source, parser=self.loader)\n",
    "            if document_ids:\n",
    "                if replace:\n",
    "                    VectorStore.purge_vector_store_documents(source=self.source, parser=self.loader)\n",
    "                else:\n",
    "                    print(f\"Documents already exist. Set replace=True to overwrite.\")\n",
    "                    return\n",
    "\n",
    "            # Extract and Load Documents\n",
    "            if not documents:\n",
    "                documents = self.extract()\n",
    "            \n",
    "            for document in documents: \n",
    "                document.metadata = {\"source\": self.source, \"parser\": self.loader}\n",
    "            \n",
    "            VectorStore.push_vector_store_documents(documents=documents, embeddings=embeddings)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA LOS MÓDULOS DE PROCESAMIENTO DE PDFs:\n",
    "# Prueba de la interfaz personalizada para los módulos de procesamiento de PDFs.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Define the PDF to load from the PDFs folder\n",
    "path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "\n",
    "# Define the Embedder\n",
    "# Available models: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3', 'mistral-embed'\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "Embedder = MyEmbedder(embedding_model)\n",
    "\n",
    "# Define the Vector Store\n",
    "# Available providers: 'chromadb', 'mongodb'\n",
    "db_provider = \"mongodb\"\n",
    "VectorStore = MyVectorStore(provider=db_provider, Embedder=Embedder)\n",
    "\n",
    "# Define the PDF Loader\n",
    "# Available loaders: 'PyMuPDF', 'UnstructuredLocal', 'UnstructuredCloud', 'AzureDocIntLocal', 'AzureDocIntCloud', 'LlamaParse'\n",
    "pdf_loader = \"PyMuPDF\"\n",
    "PdfLoader = MyPdfLoader(path=path, loader=pdf_loader)\n",
    "\n",
    "# Extract and Load the PDF data into the vector store\n",
    "PdfLoader.load(VectorStore=VectorStore, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flujo de Carga de Documentos PDF\n",
    "\n",
    "En esta sección, definimos el flujo de carga que se encarga de procesar iterativamente todos los PDFs, considerando todas las combinaciones posibles de Loader, Embedder y Vector Store. Este flujo es crucial para llenar nuestras bases de datos de vectores, que servirán como fuente de conocimiento para nuestro conjunto de RAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## FLUJO DE CARGA DE DOCUMENTOS PDF:\n",
    "# Procesamiento y carga de los documentos PDF de manera iterativa para todas \n",
    "# las combinaciones de los módulos de nuestro estudio.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.getLogger('requests').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('urllib3').setLevel(logging.CRITICAL)\n",
    "\n",
    "# For all the pdfs\n",
    "# Available PDFs: 'Attention Is All You Need.pdf', 'CODIMUR 50.pdf', 'Orden ayudas publicas.pdf'\n",
    "pdfs_list = [r\"../PDFs/Attention Is All You Need.pdf\", r\"../PDFs/CODIMUR 50.pdf\", r\"../PDFs/Orden Ayudas Publicas.pdf\"]\n",
    "for path in pdfs_list:\n",
    "       \n",
    "    # For all the pdf loaders\n",
    "    # Available loaders: 'PyMuPDF', 'UnstructuredCloud', 'AzureDocIntLocal', 'LlamaParse'\n",
    "    pdf_loader_list = [\"PyMuPDF\", \"UnstructuredCloud\", \"AzureDocIntLocal\", \"LlamaParse\"]\n",
    "    for pdf_loader in pdf_loader_list:\n",
    "\n",
    "        print(f\"Extracting {os.path.basename(path)} with {pdf_loader} ...\")\n",
    "        PdfLoader = MyPdfLoader(path=path, loader=pdf_loader)\n",
    "        documents = PdfLoader.extract()\n",
    "        print(f\"Extracting {os.path.basename(path)} with {pdf_loader} ... DONE\")\n",
    "\n",
    "        # For all the embedding models\n",
    "        # Available models: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3'\n",
    "        embedding_models_list = [\"nomic-embed-text\", \"snowflake-arctic-embed-335m\", \"text-embedding-3-large\", \"voyage-3\"]\n",
    "        for embedding_model in embedding_models_list:\n",
    "\n",
    "            print(f\"Embedding {os.path.basename(path)} with {embedding_model} ...\")\n",
    "            Embedder = MyEmbedder(embedding_model)\n",
    "            embeddings = [Embedder.get_embedder().embed_query(doc.page_content) for doc in documents]\n",
    "            print(f\"Embedding {os.path.basename(path)} with {embedding_model} ... DONE\")\n",
    "\n",
    "            # For all the database providers\n",
    "            # Available providers: 'chromadb', 'mongodb'\n",
    "            db_providers_list = [\"chromadb\", \"mongodb\"]\n",
    "            for db_provider in db_providers_list:\n",
    "\n",
    "                print(f\"Loading {os.path.basename(path)} into {db_provider} ...\")\n",
    "                VectorStore = MyVectorStore(provider=db_provider, Embedder=Embedder)\n",
    "                PdfLoader.load(VectorStore=VectorStore, documents=documents, embeddings=embeddings, replace=False)\n",
    "                print(f\"Loading {os.path.basename(path)} into {db_provider} ... DONE\")\n",
    "                \n",
    "                print(f\"Processing {os.path.basename(path)} with {pdf_loader} and {embedding_model} into {db_provider} ... DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz para el Flujo RAG\n",
    "\n",
    "En esta sección, definimos la interfaz del flujo RAG, que permite construir y ejecutar fácilmente un conjunto de RAGs. Este flujo toma como entrada los módulos configurados previamente, como el procesador de PDFs, el modelo de embeddings, la base de datos de vectores y el modelo de lenguaje. Además, permite definir parámetros como el número de resultados a recuperar (top k) y la temperatura del modelo de lenguaje para personalizar las respuestas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA EL FLUJO RAG:\n",
    "# Interfaz personalizada para definir de manera dinámica el flujo de RAG a \n",
    "# partir de sus componentes.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class MyRAGChain:\n",
    "\n",
    "    def __init__(self, source: str, pdf_loader: str, embedding_model: str, db_provider: str, llm_model: str, k: int = 5, temperature: float = 0.5):\n",
    "        self.source = source\n",
    "        self.pdf_loader = pdf_loader\n",
    "        self.embedding_model = embedding_model\n",
    "        self.db_provider = db_provider\n",
    "        self.llm_model = llm_model\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def get_rag_chain(self):\n",
    "        \n",
    "        # Document Formatting\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        ## Prompt Template for RAG\n",
    "        RAG_TEMPLATE = \"\"\"\n",
    "        You are an assistant for question-answering tasks. \n",
    "        Use only the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Answer the following question:\n",
    "\n",
    "        {question}\"\"\"\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "        # Retriever and Top K\n",
    "        Embedder = MyEmbedder(model=self.embedding_model)\n",
    "        VectorStore = MyVectorStore(provider=self.db_provider, Embedder=Embedder)\n",
    "        retriever = MyRetriever(k=self.k, source=self.source, parser=self.pdf_loader, VectorStore=VectorStore).get_retriever()\n",
    "\n",
    "        # Language Model and Temperature\n",
    "        llm = MyLLM(model=self.llm_model, temperature=self.temperature).get_llm()\n",
    "\n",
    "        # Langchain Pipeline\n",
    "        qa_chain = (\n",
    "            {\n",
    "                \"retrievals\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | RunnableMap(\n",
    "                {\n",
    "                    \"retrievals\": lambda inputs: inputs[\"retrievals\"],\n",
    "                    \"answer\": (\n",
    "                        {\n",
    "                            \"context\": lambda inputs: format_docs(inputs[\"retrievals\"]),\n",
    "                            \"question\": lambda inputs: inputs[\"question\"],\n",
    "                        }\n",
    "                        | rag_prompt\n",
    "                        | llm\n",
    "                        | StrOutputParser()\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "            | (lambda outputs: {\"retrievals\": outputs[\"retrievals\"], \"answer\": outputs[\"answer\"]})\n",
    "        )\n",
    "\n",
    "        return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## INTERFAZ PARA EL FLUJO RAG:\n",
    "# Prueba de la interfaz personalizada para el flujo de RAG.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# Define the embedding model to use\n",
    "# Options: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3'\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "\n",
    "# Define the database provider to use\n",
    "# Options: 'chromadb', 'mongodb'\n",
    "db_provider = \"mongodb\"\n",
    "\n",
    "# Define the LLM model to use\n",
    "# Options: 'gpt-4o', 'claude-3-5-sonnet', 'gemini-1.5-pro', gemma2-9b, llama3-1-8b, phi3-5\n",
    "llm_model, temperature = \"gpt-4o\", 0.5\n",
    "\n",
    "# Define the PDF Loader to use\n",
    "# Options: 'PyMuPDF', 'UnstructuredCloud', 'AzureDocIntLocal', 'LlamaParse'\n",
    "pdf_loader = \"PyMuPDF\"\n",
    "\n",
    "# Define the PDF to query\n",
    "# Options: 'Attention Is All You Need.pdf', 'CODIMUR 50.pdf', 'Orden Ayudas Publicas.pdf'\n",
    "k, source = 5, \"Attention Is All You Need.pdf\"\n",
    "\n",
    "# Define the RAG Chain\n",
    "rag_chain = MyRAGChain(\n",
    "    source=source, \n",
    "    pdf_loader=pdf_loader, \n",
    "    embedding_model=embedding_model, \n",
    "    db_provider=db_provider, \n",
    "    llm_model=llm_model, \n",
    "    k=k, \n",
    "    temperature=temperature\n",
    ").get_rag_chain()\n",
    "\n",
    "# Invoke the RAG Chain\n",
    "quetion = \"What is a Transformer?\"\n",
    "output = rag_chain.invoke(quetion)\n",
    "\n",
    "# Print the RAG Chain output\n",
    "print(output[\"answer\"])\n",
    "print(output[\"retrievals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flujo de Evaluación con DeepEval y RAGAs\n",
    "\n",
    "En esta sección, definimos el flujo de evaluación, que itera sobre todos nuestros pipelines RAG aplicándolos a las consultas del dataset de evaluación. El objetivo es calcular las métricas RAGAs, que permiten medir el rendimiento de los distintos flujos de RAG en función de las respuestas generadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## FLUJO DE EVALUACIÓN CON DEEPEVAL Y RAGAS:\n",
    "# Construcción sintetica de los datasets de evaluación con DeepEval.\n",
    "## ----------------------------------------------------------------------------\n",
    "# Nota: El rendimiento no ha sido el esperado, por lo que se ha decidido\n",
    "# generar los datasets de evaluación de manera manual.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "# PDF Options: 'Attention Is All You Need.pdf', 'CODIMUR 50.pdf', 'Orden Ayudas Publicas.pdf'\n",
    "source = \"Attention Is All You Need.pdf\"\n",
    "\n",
    "# PDF Loader Options: 'PyMuPDF', 'UnstructuredCloud', 'AzureDocIntLocal', 'LlamaParse'\n",
    "pdf_loader = \"PyMuPDF\"\n",
    "\n",
    "# Database Provider Options: 'chromadb', 'mongodb'\n",
    "db_provider = \"mongodb\"\n",
    "\n",
    "# Embedding Model Options: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3'\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "\n",
    "# Select your context to build your dataset from\n",
    "VectorStore = MyVectorStore(provider=db_provider, Embedder=MyEmbedder(embedding_model))\n",
    "vector_store = VectorStore.get_vector_store()\n",
    "documents = list(vector_store._collection.find({\"source\": source, \"parser\": pdf_loader}, {\"text\": 1}))\n",
    "\n",
    "contexts = [x['text'] for x in documents]\n",
    "contexts = [contexts[i:i + 5] for i in range(0, len(contexts), 5)]\n",
    "\n",
    "# Generate Goldens a.k.a. Question & Expected Outputs\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "synthesizer = Synthesizer(model=\"gpt-4o\", max_concurrent=1)\n",
    "synthesizer.generate_goldens_from_contexts(contexts=contexts, include_expected_output=True, max_goldens_per_context=2)\n",
    "synthesizer.save_as(file_type='json', directory=\"../DeepEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## FLUJO DE EVALUACIÓN CON DEEPEVAL Y RAGAS:\n",
    "# Interfaz de evaluación para Gemini Flash 1.5\n",
    "# DeepEval requiere de una interfaz distinta a la proporcionada por la \n",
    "# librería LangChain.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## FLUJO DE EVALUACIÓN CON DEEPEVAL Y RAGAS:\n",
    "# Unidad de evaluación de las métricas RAGAs utilizando Gemini Flash 1.5\n",
    "# Dado un pipeline de RAG y un test case, evalua el pipeline y devuelve \n",
    "# los resultados.\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "\n",
    "def gemini_evaluate_test_case(RAGChain: MyRAGChain, test_case: dict, replace: bool = False):\n",
    "\n",
    "    # If replace=False and already evaluated do not evaluate again\n",
    "    evaluation_json = f\"../DeepEval/{RAGChain.source.split('.')[0]} - Evaluation.json\"\n",
    "    with open(evaluation_json, 'r') as f:\n",
    "        existing_evaluation_list = json.load(f)\n",
    "    \n",
    "    evaluation_item = [evaluation_item for evaluation_item in existing_evaluation_list \n",
    "                       if evaluation_item['input'] == test_case['input']\n",
    "                       and evaluation_item['llm_model'] == RAGChain.llm_model\n",
    "                       and evaluation_item['embedding_model'] == RAGChain.embedding_model\n",
    "                       and evaluation_item['pdf_loader'] == RAGChain.pdf_loader\n",
    "                       and evaluation_item['db_provider'] == RAGChain.db_provider]\n",
    "    \n",
    "    \n",
    "    if evaluation_item and not replace:\n",
    "        return evaluation_item[0]\n",
    "    \n",
    "    elif evaluation_item and replace:\n",
    "        existing_evaluation_list = [item for item in existing_evaluation_list \n",
    "                                    if not (item['input'] == evaluation_item[0]['input']\n",
    "                                    and item['llm_model'] == RAGChain.llm_model\n",
    "                                    and item['embedding_model'] == RAGChain.embedding_model\n",
    "                                    and item['pdf_loader'] == RAGChain.pdf_loader\n",
    "                                    and item['db_provider'] == RAGChain.db_provider)]\n",
    "    \n",
    "    # Run the RAG Chain to generate the real output\n",
    "    output = RAGChain.get_rag_chain().invoke(test_case['input'])\n",
    "\n",
    "    # Build the LLM Test Case\n",
    "    llm_test_case = LLMTestCase(\n",
    "        input=test_case['input'],\n",
    "        actual_output=output['answer'],\n",
    "        expected_output=test_case['expected_output'],\n",
    "        retrieval_context=[doc.page_content for doc in output['retrievals']]\n",
    "    )\n",
    "\n",
    "    # Evaluate the LLM Test Case\n",
    "    evaluation_model =  CustomGeminiFlash()\n",
    "    metric_AnswerRelevancyMetric = AnswerRelevancyMetric(threshold=0.7, model=evaluation_model, include_reason=False)\n",
    "    metric_FaithfulnessMetric = FaithfulnessMetric(threshold=0.7, model=evaluation_model, include_reason=False)\n",
    "    metric_ContextualRecallMetric = ContextualRecallMetric(threshold=0.7, model=evaluation_model, include_reason=False)\n",
    "    metric_ContextualPrecisionMetric = ContextualPrecisionMetric(threshold=0.7, model=evaluation_model, include_reason=False)\n",
    "\n",
    "    evaluation_results = evaluate(\n",
    "        [llm_test_case], \n",
    "        [metric_AnswerRelevancyMetric, metric_FaithfulnessMetric, metric_ContextualRecallMetric, metric_ContextualPrecisionMetric], \n",
    "        verbose_mode=False,\n",
    "        print_results=False\n",
    "    )\n",
    "    \n",
    "    evaluation_scores = [{\"metric\":metric.name, \"score\":metric.score} for metric in evaluation_results.test_results[0].metrics_data]\n",
    "    average_score = sum(item['score'] for item in evaluation_scores) / len(evaluation_scores)\n",
    "\n",
    "    # Save Evaluation Results\n",
    "    evaluation_item = {\n",
    "        \"input\": test_case['input'],\n",
    "        \"actual_output\": output['answer'],\n",
    "        \"expected_output\": test_case['expected_output'],\n",
    "        \"retrieval_context\": [doc.page_content for doc in output['retrievals']],\n",
    "        \"context\": test_case['context'],\n",
    "        \"source\": source,\n",
    "        \"pdf_loader\": pdf_loader,\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"db_provider\": db_provider,\n",
    "        \"llm_model\": llm_model,\n",
    "        \"Contextual Precision\": round(next(item['score'] for item in evaluation_scores if item['metric'] == 'Contextual Precision'), 3),\n",
    "        \"Contextual Recall\": round(next(item['score'] for item in evaluation_scores if item['metric'] == 'Contextual Recall'), 3),\n",
    "        \"Answer Relevancy\": round(next(item['score'] for item in evaluation_scores if item['metric'] == 'Answer Relevancy'), 3),\n",
    "        \"Faithfulness\": round(next(item['score'] for item in evaluation_scores if item['metric'] == 'Faithfulness'), 3),\n",
    "        \"RAGAs Average\": round(average_score, 3)\n",
    "    }\n",
    "\n",
    "    existing_evaluation_list.extend([evaluation_item])\n",
    "                    \n",
    "    with open(evaluation_json, 'w') as f:\n",
    "        json.dump(existing_evaluation_list, f)\n",
    "    \n",
    "    return evaluation_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------------------------------------------\n",
    "## FLUJO DE EVALUACIÓN CON DEEPEVAL Y RAGAS:\n",
    "# Evaluación de los pipelines de RAG con Gemini Flash 1.5\n",
    "## ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# For all the pdfs\n",
    "# Available PDFs: 'Attention Is All You Need.pdf', 'CODIMUR 50.pdf', 'Orden Ayudas Publicas.pdf'\n",
    "pdfs_list = [r\"../PDFs/Attention Is All You Need.pdf\", r\"../PDFs/CODIMUR 50.pdf\", r\"../PDFs/Orden ayudas publicas.pdf\"]\n",
    "for path in pdfs_list:\n",
    "\n",
    "    source = os.path.basename(path)\n",
    "    file_name = source.split(\".\")[0]\n",
    "\n",
    "    dataset_json = f\"../DeepEval/{file_name} - Dataset.json\"\n",
    "    evaluation_json = f\"../DeepEval/{file_name} - Evaluation.json\"\n",
    "\n",
    "    if not os.path.exists(evaluation_json):\n",
    "        with open(evaluation_json, 'w') as f:\n",
    "            json.dump([], f)\n",
    "      \n",
    "    # For all the pdf loaders\n",
    "    # Available loaders: 'PyMuPDF', 'UnstructuredCloud', 'AzureDocIntLocal', 'LlamaParse'\n",
    "    pdf_loader_list = [\"PyMuPDF\", \"UnstructuredCloud\", \"AzureDocIntLocal\", \"LlamaParse\"]\n",
    "    for pdf_loader in pdf_loader_list:\n",
    "\n",
    "        # For all the embedding models\n",
    "        # Available models: 'nomic-embed-text', 'snowflake-arctic-embed-335m', 'text-embedding-3-large', 'voyage-3'\n",
    "        embedding_models_list = [\"nomic-embed-text\", \"snowflake-arctic-embed-335m\", \"text-embedding-3-large\", \"voyage-3\"]\n",
    "        for embedding_model in embedding_models_list:\n",
    "\n",
    "            # For all the database providers\n",
    "            # Available providers: 'chromadb', 'mongodb'\n",
    "            db_providers_list = [\"chromadb\", \"mongodb\"]\n",
    "            for db_provider in db_providers_list:\n",
    "\n",
    "                # For all the language models\n",
    "                # Available models: 'gpt-4o', 'claude-3-5-sonnet', 'gemini-1.5-pro', 'gemma2-9b', 'llama3-1-8b', 'phi3-5'\n",
    "                llm_model_list = [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\", \"gemma2-9b\", \"llama3-1-8b\", \"phi3-5\"]\n",
    "                for llm_model in llm_model_list:\n",
    "\n",
    "                    print(f\"Evaluating {llm_model} on {os.path.basename(path)} with {pdf_loader}, {embedding_model} and {db_provider} ...\")\n",
    "\n",
    "                    # --------------------------\n",
    "                    # Instantiate the RAG Chain\n",
    "                    # --------------------------\n",
    "                    RAGChain = MyRAGChain(\n",
    "                        source=source, \n",
    "                        pdf_loader=pdf_loader, \n",
    "                        embedding_model=embedding_model, \n",
    "                        db_provider=db_provider, \n",
    "                        llm_model=llm_model, \n",
    "                        k=5, \n",
    "                        temperature=0.5\n",
    "                    )\n",
    "                    # --------------------------\n",
    "                    \n",
    "                    # Load the evaluation dataset\n",
    "                    with open(dataset_json, 'r') as f:\n",
    "                        dataset_list = json.load(f)\n",
    "                    \n",
    "                    # For all the test cases in the dataset\n",
    "                    for ii, test_case in enumerate(dataset_list):\n",
    "                        # --------------------------\n",
    "                        # Evaluate the RAG Chain\n",
    "                        # --------------------------\n",
    "                        try:\n",
    "                            evaluation_item = gemini_evaluate_test_case(RAGChain=RAGChain, test_case=test_case, replace=False)\n",
    "                            print(f\"{ii+1} de {len(dataset_list)}:: {llm_model} on {os.path.basename(path)} with {pdf_loader}, {embedding_model} and {db_provider}\")\n",
    "                            print(f\"Question: {evaluation_item['input']}\")\n",
    "                            print(f\"Answer: {evaluation_item['actual_output']}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {e}\")\n",
    "                            continue\n",
    "                        # --------------------------\n",
    "\n",
    "                    print(f\"Evaluating {llm_model} on {os.path.basename(path)} with {pdf_loader}, {embedding_model} and {db_provider} ... DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los Resultados - Pendiente ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"../PDFs/Attention Is All You Need.pdf\"\n",
    "source = os.path.basename(pdf_path)\n",
    "file_name = source.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_json = f\"../DeepEval/{file_name} - Evaluation.json\"\n",
    "with open(evaluation_json, 'r') as f:\n",
    "    evaluation_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_average(evaluation_list: list, key: str, value: str):\n",
    "    filtered_list = [x for x in evaluation_list if x[key] == value]\n",
    "\n",
    "    contextual_precision_values = [x['Contextual Precision'] for x in filtered_list if not np.isnan(x['Contextual Precision'])]\n",
    "    contextual_recall_values = [x['Contextual Recall'] for x in filtered_list if not np.isnan(x['Contextual Recall'])]\n",
    "    answer_relevancy_values = [x['Answer Relevancy'] for x in filtered_list if not np.isnan(x['Answer Relevancy'])]\n",
    "    faithfulness_values = [x['Faithfulness'] for x in filtered_list if not np.isnan(x['Faithfulness'])]\n",
    "    ragas_average_values = [x['RAGAs Average'] for x in filtered_list if not np.isnan(x['RAGAs Average'])]\n",
    "\n",
    "    contextual_precision = round(sum(contextual_precision_values) / len(contextual_precision_values), 3)\n",
    "    contextual_recall = round(sum(contextual_recall_values) / len(contextual_recall_values), 3)\n",
    "    answer_relevancy = round(sum(answer_relevancy_values) / len(answer_relevancy_values), 3)\n",
    "    faithfulness = round(sum(faithfulness_values) / len(faithfulness_values), 3)\n",
    "    ragas_average = round(sum(ragas_average_values) / len(ragas_average_values), 3)\n",
    "\n",
    "    return [contextual_precision, contextual_recall, answer_relevancy, faithfulness, ragas_average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Loaders - Overall Average\n",
    "# -----------------------------------\n",
    "\n",
    "PyMuPDF_Results = overall_average(evaluation_list, 'pdf_loader', \"PyMuPDF\")\n",
    "print(PyMuPDF_Results)\n",
    "\n",
    "AzureDocIntLocal_Results = overall_average(evaluation_list, 'pdf_loader', \"AzureDocIntLocal\")\n",
    "print(AzureDocIntLocal_Results)\n",
    "\n",
    "LlamaParse_Results = overall_average(evaluation_list, 'pdf_loader', \"LlamaParse\")\n",
    "print(LlamaParse_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Embedders - Overall Average\n",
    "# -----------------------------------\n",
    "\n",
    "NomicEmbedText_Results = overall_average(evaluation_list, 'embedding_model', \"nomic-embed-text\")\n",
    "print(NomicEmbedText_Results)\n",
    "\n",
    "TextEmbedding3Large_Results = overall_average(evaluation_list, 'embedding_model', \"text-embedding-3-large\")\n",
    "print(TextEmbedding3Large_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Databases - Overall Average\n",
    "# -----------------------------------\n",
    "\n",
    "Chromadb_Results = overall_average(evaluation_list, 'db_provider', \"chromadb\")\n",
    "print(Chromadb_Results)\n",
    "\n",
    "Mongodb_Results = overall_average(evaluation_list, 'db_provider', \"mongodb\")\n",
    "print(Mongodb_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Language Models - Overall Average\n",
    "# -----------------------------------\n",
    "\n",
    "GPT4o_Results = overall_average(evaluation_list, 'llm_model', \"gpt-4o\")\n",
    "print(GPT4o_Results)\n",
    "\n",
    "Claude35Sonnet_Results = overall_average(evaluation_list, 'llm_model', \"claude-3-5-sonnet\")\n",
    "print(Claude35Sonnet_Results)\n",
    "\n",
    "Gemini15Pro_Results = overall_average(evaluation_list, 'llm_model', \"gemini-1.5-pro\")\n",
    "print(Gemini15Pro_Results)\n",
    "\n",
    "Gemma29b_Results = overall_average(evaluation_list, 'llm_model', \"gemma2-9b\")\n",
    "print(Gemma29b_Results)\n",
    "\n",
    "Llama318b_Results = overall_average(evaluation_list, 'llm_model', \"llama3-1-8b\")\n",
    "print(Llama318b_Results)\n",
    "\n",
    "Phi35_Results = overall_average(evaluation_list, 'llm_model', \"phi3-5\")\n",
    "print(Phi35_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_average(evaluation_list: list, env: str):\n",
    "    pdf_loaders_local, pdf_loaders_cloud = [\"PyMuPDF\", \"AzureDocIntLocal\"], [\"LlamaParse\"]\n",
    "    embedding_models_local, embedding_models_cloud = [\"nomic-embed-text\"], [\"text-embedding-3-large\"]\n",
    "    db_providers_local, db_providers_cloud = [\"chromadb\"], [\"mongodb\"]\n",
    "    llm_models_local, llm_models_cloud = [\"gemma2-9b\", \"llama3-1-8b\", \"phi3-5\"], [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\"]\n",
    "    \n",
    "    if env == \"local\":\n",
    "        filtered_list = [x for x in evaluation_list\n",
    "                                if (x['pdf_loader'] in pdf_loaders_local)\n",
    "                                and (x['embedding_model'] in embedding_models_local)\n",
    "                                and (x['db_provider'] in db_providers_local)\n",
    "                                and (x['llm_model'] in llm_models_local)]\n",
    "    \n",
    "    elif env == \"cloud\":\n",
    "        filtered_list = [x for x in evaluation_list\n",
    "                                if (x['pdf_loader'] in pdf_loaders_cloud)\n",
    "                                and (x['embedding_model'] in embedding_models_cloud)\n",
    "                                and (x['db_provider'] in db_providers_cloud)\n",
    "                                and (x['llm_model'] in llm_models_cloud)]\n",
    "\n",
    "\n",
    "    contextual_precision_values = [x['Contextual Precision'] for x in filtered_list if not np.isnan(x['Contextual Precision'])]\n",
    "    contextual_recall_values = [x['Contextual Recall'] for x in filtered_list if not np.isnan(x['Contextual Recall'])]\n",
    "    answer_relevancy_values = [x['Answer Relevancy'] for x in filtered_list if not np.isnan(x['Answer Relevancy'])]\n",
    "    faithfulness_values = [x['Faithfulness'] for x in filtered_list if not np.isnan(x['Faithfulness'])]\n",
    "    ragas_average_values = [x['RAGAs Average'] for x in filtered_list if not np.isnan(x['RAGAs Average'])]\n",
    "\n",
    "    contextual_precision = round(sum(contextual_precision_values) / len(contextual_precision_values), 3)\n",
    "    contextual_recall = round(sum(contextual_recall_values) / len(contextual_recall_values), 3)\n",
    "    answer_relevancy = round(sum(answer_relevancy_values) / len(answer_relevancy_values), 3)\n",
    "    faithfulness = round(sum(faithfulness_values) / len(faithfulness_values), 3)\n",
    "    ragas_average = round(sum(ragas_average_values) / len(ragas_average_values), 3)\n",
    "\n",
    "    return [contextual_precision, contextual_recall, answer_relevancy, faithfulness, ragas_average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Local vs Cloud Average\n",
    "# -----------------------------------\n",
    "\n",
    "Local_Results = env_average(evaluation_list, \"local\")   \n",
    "print(Local_Results)\n",
    "\n",
    "Cloud_Results = env_average(evaluation_list, \"cloud\")\n",
    "print(Cloud_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_within_environment(evaluation_list: list, key: str, value: str):\n",
    "    pdf_loaders_local, pdf_loaders_cloud = [\"PyMuPDF\", \"AzureDocIntLocal\"], [\"LlamaParse\"]\n",
    "    embedding_models_local, embedding_models_cloud = [\"nomic-embed-text\"], [\"text-embedding-3-large\"]\n",
    "    db_providers_local, db_providers_cloud = [\"chromadb\"], [\"mongodb\"]\n",
    "    llm_models_local, llm_models_cloud = [\"gemma2-9b\", \"llama3-1-8b\", \"phi3-5\"], [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\"]\n",
    "\n",
    "    local_list = pdf_loaders_local + embedding_models_local + db_providers_local + llm_models_local\n",
    "    cloud_list = pdf_loaders_cloud + embedding_models_cloud + db_providers_cloud + llm_models_cloud\n",
    "\n",
    "    if value in local_list:\n",
    "        env_filtered_list = [x for x in evaluation_list\n",
    "                                if (x['pdf_loader'] in pdf_loaders_local)\n",
    "                                and (x['embedding_model'] in embedding_models_local)\n",
    "                                and (x['db_provider'] in db_providers_local)\n",
    "                                and (x['llm_model'] in llm_models_local)]\n",
    "    \n",
    "    elif value in cloud_list:\n",
    "        env_filtered_list = [x for x in evaluation_list\n",
    "                                if (x['pdf_loader'] in pdf_loaders_cloud)\n",
    "                                and (x['embedding_model'] in embedding_models_cloud)\n",
    "                                and (x['db_provider'] in db_providers_cloud)\n",
    "                                and (x['llm_model'] in llm_models_cloud)]\n",
    "\n",
    "    filtered_list = [x for x in env_filtered_list if x[key] == value]\n",
    "\n",
    "    contextual_precision_values = [x['Contextual Precision'] for x in filtered_list if not np.isnan(x['Contextual Precision'])]\n",
    "    contextual_recall_values = [x['Contextual Recall'] for x in filtered_list if not np.isnan(x['Contextual Recall'])]\n",
    "    answer_relevancy_values = [x['Answer Relevancy'] for x in filtered_list if not np.isnan(x['Answer Relevancy'])]\n",
    "    faithfulness_values = [x['Faithfulness'] for x in filtered_list if not np.isnan(x['Faithfulness'])]\n",
    "    ragas_average_values = [x['RAGAs Average'] for x in filtered_list if not np.isnan(x['RAGAs Average'])]\n",
    "\n",
    "    contextual_precision = round(sum(contextual_precision_values) / len(contextual_precision_values), 3)\n",
    "    contextual_recall = round(sum(contextual_recall_values) / len(contextual_recall_values), 3)\n",
    "    answer_relevancy = round(sum(answer_relevancy_values) / len(answer_relevancy_values), 3)\n",
    "    faithfulness = round(sum(faithfulness_values) / len(faithfulness_values), 3)\n",
    "    ragas_average = round(sum(ragas_average_values) / len(ragas_average_values), 3)\n",
    "\n",
    "    return [contextual_precision, contextual_recall, answer_relevancy, faithfulness, ragas_average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Loaders - Average Within Environment\n",
    "# -----------------------------------\n",
    "\n",
    "PyMuPDF_EnvResults = average_within_environment(evaluation_list, 'pdf_loader', \"PyMuPDF\")\n",
    "print(PyMuPDF_EnvResults)\n",
    "\n",
    "AzureDocIntLocal_EnvResults = average_within_environment(evaluation_list, 'pdf_loader', \"AzureDocIntLocal\")\n",
    "print(AzureDocIntLocal_EnvResults)\n",
    "\n",
    "LlamaParse_EnvResults = average_within_environment(evaluation_list, 'pdf_loader', \"LlamaParse\")\n",
    "print(LlamaParse_EnvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Embedders - Average Within Environment\n",
    "# -----------------------------------\n",
    "\n",
    "NomicEmbedText_EnvResults = average_within_environment(evaluation_list, 'embedding_model', \"nomic-embed-text\")\n",
    "print(NomicEmbedText_EnvResults)\n",
    "\n",
    "TextEmbedding3Large_EnvResults = average_within_environment(evaluation_list, 'embedding_model', \"text-embedding-3-large\")\n",
    "print(TextEmbedding3Large_EnvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Databases - Average Within Environment\n",
    "# -----------------------------------\n",
    "\n",
    "Chromadb_EnvResults = average_within_environment(evaluation_list, 'db_provider', \"chromadb\")\n",
    "print(Chromadb_EnvResults)\n",
    "\n",
    "Mongodb_EnvResults = average_within_environment(evaluation_list, 'db_provider', \"mongodb\")\n",
    "print(Mongodb_EnvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Language Models - Average Within Environment\n",
    "# -----------------------------------\n",
    "\n",
    "GPT4o_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"gpt-4o\")\n",
    "print(GPT4o_EnvResults)\n",
    "\n",
    "Claude35Sonnet_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"claude-3-5-sonnet\")\n",
    "print(Claude35Sonnet_EnvResults)\n",
    "\n",
    "Gemini15Pro_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"gemini-1.5-pro\")\n",
    "print(Gemini15Pro_EnvResults)\n",
    "\n",
    "Gemma29b_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"gemma2-9b\")\n",
    "print(Gemma29b_EnvResults)\n",
    "\n",
    "Llama318b_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"llama3-1-8b\")\n",
    "print(Llama318b_EnvResults)\n",
    "\n",
    "Phi35_EnvResults = average_within_environment(evaluation_list, 'llm_model', \"phi3-5\")\n",
    "print(Phi35_EnvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
